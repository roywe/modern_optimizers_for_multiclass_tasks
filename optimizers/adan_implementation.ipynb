{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdanOptimizer(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, betas=(0.02, 0.08, 0.01), epsilon=1e-8, weight_decay = 0, \n",
    "                 restart_cond: callable = None):\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr = learning_rate,\n",
    "            betas = betas,\n",
    "            eps = epsilon,\n",
    "            weight_decay = weight_decay,\n",
    "            restart_cond = restart_cond\n",
    "        )\n",
    "        \n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Weights update using Adam.\n",
    "\n",
    "          m_k = (1-beta1) * m_k-1 + (beta1) * g_k\n",
    "          v_k = (1-beta2) * v_k-1 + (beta2) * (g_k-g_k_-1)\n",
    "          n_k = (1-beta3) * n_k-1 + (beta3) * (g_k+(1-beta2)(g_k-g_k_-1))^2\n",
    "          lr_k = lr/(n_k+epsilon)^0.5\n",
    "          w_k+1 = (1+wd)\n",
    "          w_k+1 = (1+weight_decay*lr)^-1 (w_k - lr_k(m_k + (1-beta2)v_k))\n",
    "        \"\"\"\n",
    "                \n",
    "        for group in self.param_groups:\n",
    "\n",
    "            lr = group['lr']\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            weight_decay = group['weight_decay']\n",
    "            eps = group['eps']\n",
    "            restart_cond = group['restart_cond']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if not (p.grad is not None):\n",
    "                    continue\n",
    "\n",
    "                data, grad = p.data, p.grad.data\n",
    "                assert not grad.is_sparse\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['prev_grad'] = torch.zeros_like(grad)\n",
    "                    state['m'] = torch.zeros_like(grad)\n",
    "                    state['v'] = torch.zeros_like(grad)\n",
    "                    state['n'] = torch.zeros_like(grad)\n",
    "\n",
    "                step, m, v, n, prev_grad = state['step'], state['m'], state['v'], state['n'], state['prev_grad']\n",
    "\n",
    "                if step > 0:\n",
    "                    prev_grad = state['prev_grad']\n",
    "                    m.mul_(1 - beta1).add_(grad, alpha = beta1)\n",
    "                    grad_diff = grad - prev_grad\n",
    "                    v.mul_(1 - beta2).add_(grad_diff, alpha = beta2)\n",
    "                    next_n = (grad + (1 - beta2) * grad_diff) ** 2\n",
    "                    n.mul_(1 - beta3).add_(next_n, alpha = beta3)\n",
    "\n",
    "                # bias correction terms\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                correct_m, correct_v, correct_n = map(lambda n: 1 / (1 - (1 - n) ** step), (beta1, beta2, beta3))\n",
    "\n",
    "                # gradient step\n",
    "\n",
    "                def grad_step_(data, m, v, n):\n",
    "                    weighted_step_size = lr / (n * correct_n).sqrt().add_(eps)# changed\n",
    "\n",
    "                    denom = 1 + weight_decay * lr\n",
    "\n",
    "                    data.addcmul_(weighted_step_size, (m * correct_m + (1 - beta2) * v * correct_v), value = -1.).div_(denom)\n",
    "\n",
    "                grad_step_(data, m, v, n)\n",
    "\n",
    "                # restart condition\n",
    "\n",
    "                # if exists(restart_cond) and restart_cond(state):\n",
    "                #     m.data.copy_(grad)\n",
    "                #     v.zero_()\n",
    "                #     n.data.copy_(grad ** 2)\n",
    "\n",
    "                #     grad_step_(data, m, v, n)\n",
    "\n",
    "                # set new incremented step\n",
    "\n",
    "                prev_grad.copy_(grad)\n",
    "                state['step'] = step\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adan_pytorch import Adan\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(16, 16),\n",
    "    nn.GELU()\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    nn.Linear(16, 16),\n",
    "    nn.GELU()\n",
    ")\n",
    "model2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = torch.randn(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdanOptimizer(\n",
    "    model.parameters(),\n",
    "    learning_rate = 1e-3,                  # learning rate (can be much higher than Adam, up to 5-10x)\n",
    "    betas = (0.02, 0.08, 0.01), # beta 1-2-3 as described in paper - author says most sensitive to beta3 tuning\n",
    "    weight_decay = 0.02         # weight decay 0.02 is optimal per author\n",
    ")\n",
    "\n",
    "optim_2 = Adan(\n",
    "    model2.parameters(),\n",
    "    lr = 1e-3,                  # learning rate (can be much higher than Adam, up to 5-10x)\n",
    "    betas = (0.02, 0.08, 0.01), # beta 1-2-3 as described in paper - author says most sensitive to beta3 tuning\n",
    "    weight_decay = 0.02         # weight decay 0.02 is optimal per author\n",
    ")\n",
    "\n",
    "# train\n",
    "for _ in range(10):\n",
    "    loss = model(model_features).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "# train\n",
    "for _ in range(10):\n",
    "    loss = model2(model_features).sum()\n",
    "    loss.backward()\n",
    "    optim_2.step()\n",
    "    optim_2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08049092,  0.2285945 , -0.07219817, -0.00345977,  0.16656336,\n",
       "        0.00050445, -0.14645079,  0.04010449, -0.1872931 , -0.06345155,\n",
       "       -0.09969974,  0.03333162,  0.18819708, -0.20207672, -0.2046853 ,\n",
       "        0.23350479], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model[0].parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08049092,  0.2285945 , -0.07219817, -0.00345977,  0.16656336,\n",
       "        0.00050445, -0.14645079,  0.04010449, -0.1872931 , -0.06345155,\n",
       "       -0.09969974,  0.03333162,  0.18819708, -0.20207672, -0.2046853 ,\n",
       "        0.23350479], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model2[0].parameters())[1].detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
