{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch import nn \n",
    "from madgrad import MADGRAD\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADGRAD_Optimizer(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic \n",
    "    Optimization.\n",
    "\n",
    "    .. _MADGRAD: https://arxiv.org/abs/2101.11075\n",
    "\n",
    "    MADGRAD is a general purpose optimizer that can be used in place of SGD or\n",
    "    Adam may converge faster and generalize better. Currently GPU-only.\n",
    "    Typically, the same learning rate schedule that is used for SGD or Adam may\n",
    "    be used. The overall learning rate is not comparable to either method and\n",
    "    should be determined by a hyper-parameter sweep.\n",
    "\n",
    "    MADGRAD requires less weight decay than other methods, often as little as\n",
    "    zero. Momentum values used for SGD or Adam's beta1 should work here also.\n",
    "\n",
    "    On sparse problems both weight_decay and momentum should be set to 0.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): \n",
    "            Iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float): \n",
    "            Learning rate (default: 1e-2).\n",
    "        momentum (float): \n",
    "            Momentum value in the range [0,1) (default: 0.9).\n",
    "        weight_decay (float): \n",
    "            Weight decay, i.e. a L2 penalty (default: 0).\n",
    "        eps (float): \n",
    "            Term added to the denominator outside of the root operation to improve \n",
    "            numerical stability. (default: 1e-6).\n",
    "            This parameter is less important in MADGRAD than in Adam. \n",
    "            On problems with very small gradients, setting this to 0 will improve convergence.\n",
    "        decouple_decay (bool):\n",
    "            Apply AdamW style decoupled weight decay (EXPERIMENTAL).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, \n",
    "        weight_decay: float = 0, eps: float = 1e-6, decouple_decay=False,\n",
    "    ):\n",
    "        if momentum < 0 or momentum >= 1:\n",
    "            raise ValueError(f\"Momentum {momentum} must be in the range [0,1)\")\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Learning rate {lr} must be non-negative\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n",
    "        if eps < 0:\n",
    "            raise ValueError(f\"Eps must be non-negative\")\n",
    "\n",
    "        defaults = dict(lr=lr, eps=eps, momentum=momentum, \n",
    "                        weight_decay=weight_decay, decouple_decay=decouple_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # step counter must be stored in state to ensure correct behavior under\n",
    "        # optimizer sharding\n",
    "        if 'k' not in self.state:\n",
    "            self.state['k'] = torch.tensor([0], dtype=torch.long)\n",
    "        k = self.state['k'].item()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            eps = group[\"eps\"]\n",
    "            lr = group[\"lr\"]\n",
    "            if lr != 0.0:\n",
    "                lr = lr + eps # For stability\n",
    "            decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            decouple_decay = group.get(\"decouple_decay\", False)\n",
    "\n",
    "            ck = 1 - momentum\n",
    "            lamb = lr * math.pow(k + 1, 0.5)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # grad = p.grad.data\n",
    "                # state = self.state[p]\n",
    "                grad = p.grad.data\n",
    "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    grad = grad.float()\n",
    "                state = self.state[p]\n",
    "\n",
    "                p_data_fp32 = p.data\n",
    "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    p_data_fp32 = p_data_fp32.float()\n",
    "\n",
    "\n",
    "                if \"grad_sum_sq\" not in state:\n",
    "                    state[\"grad_sum_sq\"] = torch.zeros_like(p_data_fp32).detach()\n",
    "                    state[\"s\"] = torch.zeros_like(p_data_fp32).detach()\n",
    "                    if momentum != 0:\n",
    "                        state[\"x0\"] = torch.clone(p_data_fp32).detach()\n",
    "\n",
    "                if momentum != 0.0 and grad.is_sparse:\n",
    "                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n",
    "\n",
    "                grad_sum_sq = state[\"grad_sum_sq\"]\n",
    "                s = state[\"s\"]\n",
    "\n",
    "                # Apply weight decay\n",
    "                if decay != 0 and not decouple_decay:\n",
    "                    if grad.is_sparse:\n",
    "                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n",
    "\n",
    "                    grad.add_(p_data_fp32, alpha=decay)\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    grad = grad.coalesce()\n",
    "                    grad_val = grad._values()\n",
    "\n",
    "                    p_masked = p_data_fp32.sparse_mask(grad)\n",
    "                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n",
    "                    s_masked = s.sparse_mask(grad)\n",
    "\n",
    "                    # Compute x_0 from other known quantities\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n",
    "                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n",
    "\n",
    "                    # Dense + sparse op\n",
    "                    grad_sq = grad * grad\n",
    "                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n",
    "                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n",
    "\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n",
    "\n",
    "                    if eps == 0:\n",
    "                        rms_masked_vals[rms_masked_vals == 0] = float('inf')\n",
    "\n",
    "                    s.add_(grad, alpha=lamb)\n",
    "                    s_masked._values().add_(grad_val, alpha=lamb)\n",
    "\n",
    "                    # update masked copy of p\n",
    "                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n",
    "                    # Copy updated masked p to dense p using an add operation\n",
    "                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n",
    "                    p.data.add_(p_masked, alpha=-1)\n",
    "                else:\n",
    "                    if momentum == 0:\n",
    "                        # Compute x_0 from other known quantities\n",
    "                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "                        x0 = p_data_fp32.addcdiv(s, rms, value=1)\n",
    "                    else:\n",
    "                        x0 = state[\"x0\"]\n",
    "\n",
    "                    # Accumulate second moments\n",
    "                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n",
    "                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "\n",
    "                    if eps == 0:\n",
    "                        rms[rms == 0] = float('inf')\n",
    "\n",
    "                    # Update s\n",
    "                    s.data.add_(grad, alpha=lamb)\n",
    "\n",
    "                    if decay != 0 and decouple_decay:\n",
    "                        p_old = p_data_fp32.clone()\n",
    "\n",
    "                    # Step\n",
    "                    if momentum == 0:\n",
    "                        p_data_fp32.copy_(x0.addcdiv(s, rms, value=-1))\n",
    "                    else:\n",
    "                        z = x0.addcdiv(s, rms, value=-1)\n",
    "\n",
    "                        # p is a moving average of z\n",
    "                        p_data_fp32.mul_(1 - ck).add_(z, alpha=ck)\n",
    "                    \n",
    "                    if decay != 0 and decouple_decay:\n",
    "                        p_data_fp32.add_(p_old, alpha=-lr*decay)\n",
    "\n",
    "                    if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
    "                        p.data.copy_(p_data_fp32)\n",
    "\n",
    "        self.state['k'] += 1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(16, 16),\n",
    "    nn.GELU()\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    nn.Linear(16, 16),\n",
    "    nn.GELU()\n",
    ")\n",
    "model2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = torch.randn(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = MADGRAD_Optimizer(\n",
    "    model.parameters(),\n",
    "    lr = 1e-3,                  # learning rate (can be much higher than Adam, up to 5-10x)\n",
    "    momentum=0.9                           # momentum = 0.9\n",
    ")\n",
    "\n",
    "optim_2 = MADGRAD(\n",
    "    model2.parameters(),\n",
    "    lr = 1e-3,                  # learning rate (can be much higher than Adam, up to 5-10x)\n",
    "    momentum=0.9                           # momentum = 0.9\n",
    ")\n",
    "\n",
    "# train\n",
    "for _ in range(10):\n",
    "    loss = model(model_features).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "# train\n",
    "for _ in range(10):\n",
    "    loss = model2(model_features).sum()\n",
    "    loss.backward()\n",
    "    optim_2.step()\n",
    "    optim_2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24592276,  0.06001983, -0.02793835,  0.13293546, -0.1924658 ,\n",
       "       -0.00853375, -0.13012622, -0.12116072, -0.22155315, -0.11364537,\n",
       "       -0.00571587,  0.05380969,  0.15654948, -0.0753729 ,  0.21154772,\n",
       "       -0.0446835 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model[0].parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24592276,  0.06001983, -0.02793835,  0.13293546, -0.1924658 ,\n",
       "       -0.00853375, -0.13012622, -0.12116072, -0.22155315, -0.11364537,\n",
       "       -0.00571587,  0.05380969,  0.15654948, -0.0753729 ,  0.21154772,\n",
       "       -0.0446835 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model2[0].parameters())[1].detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncsnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
